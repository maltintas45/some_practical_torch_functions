{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some case, we have to pick up from a tensor some columns that consist a specific value in corresponding another tensor or matrix. Here, I have implemented a function that does this process effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T12:58:18.257176Z",
     "start_time": "2021-06-18T12:58:18.253626Z"
    }
   },
   "source": [
    "This study is coded using Torch but similar functions to the required functions; $bincount$, $nonzero$, $gather$ and $repeat$ are also avaible in other NN freamworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:38:47.535079Z",
     "start_time": "2021-06-18T13:38:47.244399Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def operation_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    elapsed_ss = '{0:.4f}'.format(elapsed_time - (elapsed_mins * 60)-elapsed_secs)\n",
    "    return elapsed_mins, elapsed_secs,elapsed_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create dummy embedding tensor and it's corresponding another tensor (let' call it as '$hp$'. Do not worry about the name, it's just a name)  that keeps info bout each embedding vector. Dimension of embedding tensor is $batch\\_size$ x $max\\_seq\\_length$ x $hidden\\_dim$. Dimension of $hp$ is $batch\\_size$ x $max\\_seq\\_length$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:38:48.282403Z",
     "start_time": "2021-06-18T13:38:48.274929Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "max_seq_len = 10\n",
    "hidden_size = 9\n",
    "embedding_tensor = torch.empty(batch_size, max_seq_len, hidden_size)\n",
    "for i in range(batch_size):\n",
    "    for j in range(max_seq_len):\n",
    "        for k in range(hidden_size):\n",
    "            embedding_tensor[i,j,k] = i + j*10 + k*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:38:48.821001Z",
     "start_time": "2021-06-18T13:38:48.804072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0., 100., 200., 300., 400., 500., 600., 700., 800.],\n",
       "         [ 10., 110., 210., 310., 410., 510., 610., 710., 810.],\n",
       "         [ 20., 120., 220., 320., 420., 520., 620., 720., 820.],\n",
       "         [ 30., 130., 230., 330., 430., 530., 630., 730., 830.],\n",
       "         [ 40., 140., 240., 340., 440., 540., 640., 740., 840.],\n",
       "         [ 50., 150., 250., 350., 450., 550., 650., 750., 850.],\n",
       "         [ 60., 160., 260., 360., 460., 560., 660., 760., 860.],\n",
       "         [ 70., 170., 270., 370., 470., 570., 670., 770., 870.],\n",
       "         [ 80., 180., 280., 380., 480., 580., 680., 780., 880.],\n",
       "         [ 90., 190., 290., 390., 490., 590., 690., 790., 890.]],\n",
       "\n",
       "        [[  1., 101., 201., 301., 401., 501., 601., 701., 801.],\n",
       "         [ 11., 111., 211., 311., 411., 511., 611., 711., 811.],\n",
       "         [ 21., 121., 221., 321., 421., 521., 621., 721., 821.],\n",
       "         [ 31., 131., 231., 331., 431., 531., 631., 731., 831.],\n",
       "         [ 41., 141., 241., 341., 441., 541., 641., 741., 841.],\n",
       "         [ 51., 151., 251., 351., 451., 551., 651., 751., 851.],\n",
       "         [ 61., 161., 261., 361., 461., 561., 661., 761., 861.],\n",
       "         [ 71., 171., 271., 371., 471., 571., 671., 771., 871.],\n",
       "         [ 81., 181., 281., 381., 481., 581., 681., 781., 881.],\n",
       "         [ 91., 191., 291., 391., 491., 591., 691., 791., 891.]],\n",
       "\n",
       "        [[  2., 102., 202., 302., 402., 502., 602., 702., 802.],\n",
       "         [ 12., 112., 212., 312., 412., 512., 612., 712., 812.],\n",
       "         [ 22., 122., 222., 322., 422., 522., 622., 722., 822.],\n",
       "         [ 32., 132., 232., 332., 432., 532., 632., 732., 832.],\n",
       "         [ 42., 142., 242., 342., 442., 542., 642., 742., 842.],\n",
       "         [ 52., 152., 252., 352., 452., 552., 652., 752., 852.],\n",
       "         [ 62., 162., 262., 362., 462., 562., 662., 762., 862.],\n",
       "         [ 72., 172., 272., 372., 472., 572., 672., 772., 872.],\n",
       "         [ 82., 182., 282., 382., 482., 582., 682., 782., 882.],\n",
       "         [ 92., 192., 292., 392., 492., 592., 692., 792., 892.]],\n",
       "\n",
       "        [[  3., 103., 203., 303., 403., 503., 603., 703., 803.],\n",
       "         [ 13., 113., 213., 313., 413., 513., 613., 713., 813.],\n",
       "         [ 23., 123., 223., 323., 423., 523., 623., 723., 823.],\n",
       "         [ 33., 133., 233., 333., 433., 533., 633., 733., 833.],\n",
       "         [ 43., 143., 243., 343., 443., 543., 643., 743., 843.],\n",
       "         [ 53., 153., 253., 353., 453., 553., 653., 753., 853.],\n",
       "         [ 63., 163., 263., 363., 463., 563., 663., 763., 863.],\n",
       "         [ 73., 173., 273., 373., 473., 573., 673., 773., 873.],\n",
       "         [ 83., 183., 283., 383., 483., 583., 683., 783., 883.],\n",
       "         [ 93., 193., 293., 393., 493., 593., 693., 793., 893.]],\n",
       "\n",
       "        [[  4., 104., 204., 304., 404., 504., 604., 704., 804.],\n",
       "         [ 14., 114., 214., 314., 414., 514., 614., 714., 814.],\n",
       "         [ 24., 124., 224., 324., 424., 524., 624., 724., 824.],\n",
       "         [ 34., 134., 234., 334., 434., 534., 634., 734., 834.],\n",
       "         [ 44., 144., 244., 344., 444., 544., 644., 744., 844.],\n",
       "         [ 54., 154., 254., 354., 454., 554., 654., 754., 854.],\n",
       "         [ 64., 164., 264., 364., 464., 564., 664., 764., 864.],\n",
       "         [ 74., 174., 274., 374., 474., 574., 674., 774., 874.],\n",
       "         [ 84., 184., 284., 384., 484., 584., 684., 784., 884.],\n",
       "         [ 94., 194., 294., 394., 494., 594., 694., 794., 894.]],\n",
       "\n",
       "        [[  5., 105., 205., 305., 405., 505., 605., 705., 805.],\n",
       "         [ 15., 115., 215., 315., 415., 515., 615., 715., 815.],\n",
       "         [ 25., 125., 225., 325., 425., 525., 625., 725., 825.],\n",
       "         [ 35., 135., 235., 335., 435., 535., 635., 735., 835.],\n",
       "         [ 45., 145., 245., 345., 445., 545., 645., 745., 845.],\n",
       "         [ 55., 155., 255., 355., 455., 555., 655., 755., 855.],\n",
       "         [ 65., 165., 265., 365., 465., 565., 665., 765., 865.],\n",
       "         [ 75., 175., 275., 375., 475., 575., 675., 775., 875.],\n",
       "         [ 85., 185., 285., 385., 485., 585., 685., 785., 885.],\n",
       "         [ 95., 195., 295., 395., 495., 595., 695., 795., 895.]],\n",
       "\n",
       "        [[  6., 106., 206., 306., 406., 506., 606., 706., 806.],\n",
       "         [ 16., 116., 216., 316., 416., 516., 616., 716., 816.],\n",
       "         [ 26., 126., 226., 326., 426., 526., 626., 726., 826.],\n",
       "         [ 36., 136., 236., 336., 436., 536., 636., 736., 836.],\n",
       "         [ 46., 146., 246., 346., 446., 546., 646., 746., 846.],\n",
       "         [ 56., 156., 256., 356., 456., 556., 656., 756., 856.],\n",
       "         [ 66., 166., 266., 366., 466., 566., 666., 766., 866.],\n",
       "         [ 76., 176., 276., 376., 476., 576., 676., 776., 876.],\n",
       "         [ 86., 186., 286., 386., 486., 586., 686., 786., 886.],\n",
       "         [ 96., 196., 296., 396., 496., 596., 696., 796., 896.]],\n",
       "\n",
       "        [[  7., 107., 207., 307., 407., 507., 607., 707., 807.],\n",
       "         [ 17., 117., 217., 317., 417., 517., 617., 717., 817.],\n",
       "         [ 27., 127., 227., 327., 427., 527., 627., 727., 827.],\n",
       "         [ 37., 137., 237., 337., 437., 537., 637., 737., 837.],\n",
       "         [ 47., 147., 247., 347., 447., 547., 647., 747., 847.],\n",
       "         [ 57., 157., 257., 357., 457., 557., 657., 757., 857.],\n",
       "         [ 67., 167., 267., 367., 467., 567., 667., 767., 867.],\n",
       "         [ 77., 177., 277., 377., 477., 577., 677., 777., 877.],\n",
       "         [ 87., 187., 287., 387., 487., 587., 687., 787., 887.],\n",
       "         [ 97., 197., 297., 397., 497., 597., 697., 797., 897.]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = torch.randint(10, (batch_size, max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 0, 5, 3, 8, 7, 6, 1, 9, 8],\n",
       "        [5, 1, 2, 1, 7, 0, 2, 1, 8, 9],\n",
       "        [8, 2, 8, 7, 0, 0, 7, 7, 6, 8],\n",
       "        [6, 3, 1, 4, 4, 3, 8, 5, 4, 6],\n",
       "        [1, 3, 6, 8, 8, 0, 0, 9, 8, 8],\n",
       "        [7, 5, 1, 4, 0, 6, 2, 9, 8, 8],\n",
       "        [1, 5, 2, 2, 9, 5, 4, 1, 4, 9],\n",
       "        [0, 5, 7, 6, 4, 7, 0, 3, 3, 3]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we want to collect embedding vectors that is correspoding value of hp is equal to 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have to find which indices of $hp$ consist the value of $wanted$. Here, $wanted$ is equal to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_is,token_is = (hp==wanted).nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the rows in $hp$ has no $wanted$ value, some of them consist one or more than one. Therefore, the number of $wanted$ value in a row may vary. Because of this, we have to find max number that $wanted$ occurs in a row.  We have to store the indices in a tensor, so I initialize all this tensor by -1. Here -1 correponds the last vector of sequence and I am sure it is padding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.zeros((batch_size,torch.bincount(batch_is).max().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fill the $indices$ with corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0m 0s 0.0015ss with using gather function\n"
     ]
    }
   ],
   "source": [
    "tic=time.time()\n",
    "for batch_i, token_i in zip(batch_is,token_is):\n",
    "    m =(batch_is==batch_i).nonzero().flatten().tolist()\n",
    "    k = len(m)\n",
    "    indices[batch_i,:k]= token_is[m]\n",
    "toc=time.time()\n",
    "mins, secs, sses = operation_time(tic, toc)\n",
    "print(f'Finished in {mins}m {secs}s {sses}ss with using gather function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the indices that can be used with $gather$ function to pick up corresponding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [2., 6.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [6., 0.],\n",
       "        [2., 3.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T12:13:58.390912Z",
     "start_time": "2021-06-18T12:13:58.387868Z"
    }
   },
   "source": [
    "To use gather we have to aware three parameter, \n",
    "- input : input tensor\n",
    "- dim     : dimension along to collect values\n",
    "- indices : tensor with indices of values to collect (it needs to be same dimension with input tensor except dim axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will collect our vector on dim 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:38:55.576252Z",
     "start_time": "2021-06-18T13:38:55.570869Z"
    }
   },
   "outputs": [],
   "source": [
    "#repeat hidden_size times\n",
    "batch_size, max_seq = indices.shape \n",
    "indices_ = indices.repeat(1,hidden_size).view(batch_size,-1,max_seq).transpose(2,1).type(torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aware that, $indices$ must be type of integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:38:56.243312Z",
     "start_time": "2021-06-18T13:38:56.235589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 9])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:38:56.743878Z",
     "start_time": "2021-06-18T13:38:56.736087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [6, 6, 6, 6, 6, 6, 6, 6, 6]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [3, 3, 3, 3, 3, 3, 3, 3, 3]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:38:57.328191Z",
     "start_time": "2021-06-18T13:38:57.322924Z"
    }
   },
   "outputs": [],
   "source": [
    "reduced_embedding_tensor = torch.gather(embedding_tensor,1,indices_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:38:58.104972Z",
     "start_time": "2021-06-18T13:38:58.099890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0., 100., 200., 300., 400., 500., 600., 700., 800.],\n",
       "         [  0., 100., 200., 300., 400., 500., 600., 700., 800.]],\n",
       "\n",
       "        [[ 21., 121., 221., 321., 421., 521., 621., 721., 821.],\n",
       "         [ 61., 161., 261., 361., 461., 561., 661., 761., 861.]],\n",
       "\n",
       "        [[ 12., 112., 212., 312., 412., 512., 612., 712., 812.],\n",
       "         [  2., 102., 202., 302., 402., 502., 602., 702., 802.]],\n",
       "\n",
       "        [[  3., 103., 203., 303., 403., 503., 603., 703., 803.],\n",
       "         [  3., 103., 203., 303., 403., 503., 603., 703., 803.]],\n",
       "\n",
       "        [[  4., 104., 204., 304., 404., 504., 604., 704., 804.],\n",
       "         [  4., 104., 204., 304., 404., 504., 604., 704., 804.]],\n",
       "\n",
       "        [[ 65., 165., 265., 365., 465., 565., 665., 765., 865.],\n",
       "         [  5., 105., 205., 305., 405., 505., 605., 705., 805.]],\n",
       "\n",
       "        [[ 26., 126., 226., 326., 426., 526., 626., 726., 826.],\n",
       "         [ 36., 136., 236., 336., 436., 536., 636., 736., 836.]],\n",
       "\n",
       "        [[  7., 107., 207., 307., 407., 507., 607., 707., 807.],\n",
       "         [  7., 107., 207., 307., 407., 507., 607., 707., 807.]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_embedding_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we have done it succesfully. Let's define as all this procedure in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_up_corresponding_vectors(embedding_tensor,hp,wanted):\n",
    "    batch_size, sequence_length, hidden_size = embedding_tensor.shape\n",
    "    batch_is,token_is = (hp==wanted).nonzero(as_tuple=True)\n",
    "    indices = torch.zeros((batch_size,torch.bincount(batch_is).max().item()))\n",
    "    for batch_i, token_i in zip(batch_is,token_is):\n",
    "        m =(batch_is==batch_i).nonzero().flatten().tolist()\n",
    "        k = len(m)\n",
    "        indices[batch_i,:k]= token_is[m]\n",
    "    batch_size, max_seq = indices.shape \n",
    "    indices_ = indices.repeat(1,hidden_size).view(batch_size,-1,max_seq).transpose(2,1).type(torch.int64)\n",
    "    return torch.gather(embedding_tensor,1,indices_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:15:38.443566Z",
     "start_time": "2021-06-18T13:15:38.437933Z"
    }
   },
   "source": [
    "Let's call this function with a huge tensor to examine it's speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:39:27.106671Z",
     "start_time": "2021-06-18T13:39:00.619053Z"
    }
   },
   "outputs": [],
   "source": [
    "# dummy embedding tensor\n",
    "batch_size = 800\n",
    "max_seq_len = 58\n",
    "hidden_size = 500\n",
    "embedding_tensor = torch.empty(batch_size, max_seq_len, hidden_size)\n",
    "for i in range(batch_size):\n",
    "    for j in range(max_seq_len):\n",
    "        for k in range(hidden_size):\n",
    "            embedding_tensor[i,j,k] = i + j*10 + k*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:39:27.447275Z",
     "start_time": "2021-06-18T13:39:27.343123Z"
    }
   },
   "outputs": [],
   "source": [
    "hp = torch.randint(10, (batch_size, max_seq_len))\n",
    "wanted = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T13:40:43.752750Z",
     "start_time": "2021-06-18T13:40:43.720086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0m 0s 0.2922ss\n"
     ]
    }
   ],
   "source": [
    "tic=time.time()\n",
    "pick_up_corresponding_vectors(embedding_tensor,hp,wanted)\n",
    "toc=time.time()\n",
    "mins, secs, sses = operation_time(tic, toc)\n",
    "print(f'Finished in {mins}m {secs}s {sses}ss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know faster method does the same process, please inform me. Thanks for your attention. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
